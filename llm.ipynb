{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six\n",
        "!pip install langchain_google_genai python-docx langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NsZ1iqgca4LK",
        "outputId": "034753e3-c757-414f-c65a-bdbaa148205f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20250506\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.52 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.59)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.11.4)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain_google_genai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain_google_genai) (24.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.52->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.4-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, python-docx, google-ai-generativelanguage, langchain_google_genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain_google_genai-2.1.4 python-docx-1.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "76d19865373d4288bac60c370d98dcf9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EoA_j4E2MwEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HbcoEv4-MxOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp0nVsm7e5fk"
      },
      "outputs": [],
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "import re\n",
        "\n",
        "def extract_text_with_pdfminer(pdf_path):\n",
        "    return extract_text(pdf_path)\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    text = text.replace('\\f', ' ')\n",
        "    text = re.sub(r'\\s+\\n', '\\n', text)\n",
        "    text = re.sub(r'\\n+', '\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "def extract_sections(text):\n",
        "    text = clean_text(text)\n",
        "\n",
        "\n",
        "    pattern = r'^(Section\\s+\\d+:\\s+.*)$'\n",
        "    headings = [(m.start(), m.group(0).strip()) for m in re.finditer(pattern, text, re.MULTILINE)]\n",
        "\n",
        "    sections = {}\n",
        "    if headings:\n",
        "\n",
        "        if headings[0][0] > 0:\n",
        "            preamble = text[:headings[0][0]].strip()\n",
        "            if preamble:\n",
        "                sections[\"Title\"] = preamble\n",
        "\n",
        "\n",
        "        for i, (start, heading) in enumerate(headings):\n",
        "            end = headings[i+1][0] if i+1 < len(headings) else len(text)\n",
        "            section_text = text[start:end].strip()\n",
        "\n",
        "            content = section_text[len(heading):].strip()\n",
        "            sections[heading] = content\n",
        "    else:\n",
        "        sections[\"FullText\"] = text\n",
        "    return sections\n",
        "\n",
        "# --- Main Pipeline ---\n",
        "\n",
        "\n",
        "pdf_path = \"/content/report input 2.pdf\"\n",
        "pdf_text = extract_text_with_pdfminer(pdf_path)\n",
        "sections = extract_sections(pdf_text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "section_list_with_headings = [(heading, content) for heading, content in sections.items()]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "formatted_sections = [f\"{heading}\\n\\n{content}\" for heading, content in section_list_with_headings]\n",
        "\n",
        "\n",
        "print(formatted_sections[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0ShQ0sofN6o",
        "outputId": "3dbee1ab-584a-4003-d481-c9dd87ba98d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FullText\n",
            "\n",
            "section 1: Application Chosen\n",
            "Implemented a Reinforcement Learning (RL) agent to play the Chrome Dino game\n",
            "automatically.\n",
            "Environment includes obstacles (cacti, birds), speed increase, and fixed jump mechanics.\n",
            "Objective: Learn to survive and maximize score by choosing actions (Jump, Duck, Stand)\n",
            "optimally.\n",
            "section 2: Dataset Description\n",
            "No traditional dataset—agent learns from interactions with the environment (frames,\n",
            "rewards).\n",
            "Each data sample is a transition tuple:\n",
            " (s,a,r,s′,done)(s, a, r, s', done)(s,a,r,s′,done) where:\n",
            "sss: stacked grayscale game frames\n",
            "aaa: action taken\n",
            "rrr: reward (+1 for pass, −1 for collision)\n",
            "s′s's′: next state\n",
            "donedonedone: episode termination flag\n",
            "Internally, experiences are stored in a replay buffer.\n",
            "section 3: Recent Technique Architecture\n",
            "Used Deep Q-Network (DQN) and Quantile Regression DQN (QR-DQN) from Stable-\n",
            "Baselines3.\n",
            "Input: 4-stacked grayscale resized frames (shape: 4 × 64 × 128).\n",
            " Backbone: Convolutional Neural Network (CnnPolicy)\n",
            "3 Conv layers → Fully Connected layers → Q-values or quantile outputs.\n",
            "QR-DQN outputs multiple quantiles per action instead of a single Q-value, modeling\n",
            "reward distributions.\n",
            "section 4: Hyperparameters\n",
            "Learning rate: 1e-4\n",
            "Batch size: 32\n",
            "Replay buffer size: 100,000\n",
            "Exploration:\n",
            "Initial epsilon: 1.0\n",
            "Final epsilon: 0.02\n",
            "Exploration decay: 10% of total timesteps\n",
            "Train frequency: every 4 steps\n",
            "Target network update interval: 10,000 steps\n",
            "Discount factor γ: 0.99\n",
            "Total timesteps: 2 million\n",
            "Eval callback frequency: 20,000 steps\n",
            "section 5: Comparison of the Different Hyperparameters\n",
            "Same hyperparameters were used for both DQN and QR-DQN to ensure fair comparison.\n",
            "Main difference was the architecture: DQN uses scalar Q-values; QR-DQN uses 51\n",
            "quantiles (distributional output).\n",
            " Despite QR-DQN being more robust in general, in this constrained setting, classic DQN\n",
            "performed better.\n",
            "section 6: Metrics Used\n",
            "ep_rew_mean: Mean reward per episode (main metric for convergence)\n",
            "ep_len_mean: Mean episode length (fixed to 125 due to training cap)\n",
            "n_updates: Total number of learning updates\n",
            "loss: Bellman or quantile regression loss\n",
            "Maximum game score achieved (measured manually via evaluation episodes)\n",
            "section 7: Result Analysis and Interpretation\n",
            "DQN:\n",
            "Converged at episode reward ≈ 17\n",
            "Reached maximum score: 900\n",
            "QR-DQN:\n",
            "Converged at episode reward ≈ 13.1\n",
            "Reached maximum score: 650\n",
            "Despite QR-DQN being distribution-aware, its performance plateaued lower than DQN in\n",
            "this environment.\n",
            "Possible reasons:\n",
            "Simpler environment may not benefit from distributional modeling\n",
            "QR-DQN slower convergence due to more complex objective\n",
            "DQN’s max-over-actions Q-estimate was more decisive in fast-paced binary-reward setup\n",
            " section 8: Conclusions\n",
            "Both DQN and QR-DQN successfully learned to survive and score in the Chrome Dino\n",
            "game.\n",
            "DQN outperformed QR-DQN in final reward and max game score under identical\n",
            "hyperparameter settings.\n",
            "QR-DQN may need longer training or further tuning to show benefits in low-complexity\n",
            "environments.\n",
            "Reinforcement Learning can effectively master pixel-based games with structured rewards\n",
            "using deep Q-learning methods.\n",
            "Training logs, TensorBoard results, model weights, and evaluation GIFs available.\n",
            "Report and presentation summarize methods, metrics, and conclusions visually and\n",
            "technically.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for section in formatted_sections:\n",
        "\n",
        "    print(section)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK-BnGW0fbNP",
        "outputId": "62e46837-fdbb-4603-dffd-616500adb234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FullText\n",
            "\n",
            "section 1: Application Chosen\n",
            "Implemented a Reinforcement Learning (RL) agent to play the Chrome Dino game\n",
            "automatically.\n",
            "Environment includes obstacles (cacti, birds), speed increase, and fixed jump mechanics.\n",
            "Objective: Learn to survive and maximize score by choosing actions (Jump, Duck, Stand)\n",
            "optimally.\n",
            "section 2: Dataset Description\n",
            "No traditional dataset—agent learns from interactions with the environment (frames,\n",
            "rewards).\n",
            "Each data sample is a transition tuple:\n",
            " (s,a,r,s′,done)(s, a, r, s', done)(s,a,r,s′,done) where:\n",
            "sss: stacked grayscale game frames\n",
            "aaa: action taken\n",
            "rrr: reward (+1 for pass, −1 for collision)\n",
            "s′s's′: next state\n",
            "donedonedone: episode termination flag\n",
            "Internally, experiences are stored in a replay buffer.\n",
            "section 3: Recent Technique Architecture\n",
            "Used Deep Q-Network (DQN) and Quantile Regression DQN (QR-DQN) from Stable-\n",
            "Baselines3.\n",
            "Input: 4-stacked grayscale resized frames (shape: 4 × 64 × 128).\n",
            " Backbone: Convolutional Neural Network (CnnPolicy)\n",
            "3 Conv layers → Fully Connected layers → Q-values or quantile outputs.\n",
            "QR-DQN outputs multiple quantiles per action instead of a single Q-value, modeling\n",
            "reward distributions.\n",
            "section 4: Hyperparameters\n",
            "Learning rate: 1e-4\n",
            "Batch size: 32\n",
            "Replay buffer size: 100,000\n",
            "Exploration:\n",
            "Initial epsilon: 1.0\n",
            "Final epsilon: 0.02\n",
            "Exploration decay: 10% of total timesteps\n",
            "Train frequency: every 4 steps\n",
            "Target network update interval: 10,000 steps\n",
            "Discount factor γ: 0.99\n",
            "Total timesteps: 2 million\n",
            "Eval callback frequency: 20,000 steps\n",
            "section 5: Comparison of the Different Hyperparameters\n",
            "Same hyperparameters were used for both DQN and QR-DQN to ensure fair comparison.\n",
            "Main difference was the architecture: DQN uses scalar Q-values; QR-DQN uses 51\n",
            "quantiles (distributional output).\n",
            " Despite QR-DQN being more robust in general, in this constrained setting, classic DQN\n",
            "performed better.\n",
            "section 6: Metrics Used\n",
            "ep_rew_mean: Mean reward per episode (main metric for convergence)\n",
            "ep_len_mean: Mean episode length (fixed to 125 due to training cap)\n",
            "n_updates: Total number of learning updates\n",
            "loss: Bellman or quantile regression loss\n",
            "Maximum game score achieved (measured manually via evaluation episodes)\n",
            "section 7: Result Analysis and Interpretation\n",
            "DQN:\n",
            "Converged at episode reward ≈ 17\n",
            "Reached maximum score: 900\n",
            "QR-DQN:\n",
            "Converged at episode reward ≈ 13.1\n",
            "Reached maximum score: 650\n",
            "Despite QR-DQN being distribution-aware, its performance plateaued lower than DQN in\n",
            "this environment.\n",
            "Possible reasons:\n",
            "Simpler environment may not benefit from distributional modeling\n",
            "QR-DQN slower convergence due to more complex objective\n",
            "DQN’s max-over-actions Q-estimate was more decisive in fast-paced binary-reward setup\n",
            " section 8: Conclusions\n",
            "Both DQN and QR-DQN successfully learned to survive and score in the Chrome Dino\n",
            "game.\n",
            "DQN outperformed QR-DQN in final reward and max game score under identical\n",
            "hyperparameter settings.\n",
            "QR-DQN may need longer training or further tuning to show benefits in low-complexity\n",
            "environments.\n",
            "Reinforcement Learning can effectively master pixel-based games with structured rewards\n",
            "using deep Q-learning methods.\n",
            "Training logs, TensorBoard results, model weights, and evaluation GIFs available.\n",
            "Report and presentation summarize methods, metrics, and conclusions visually and\n",
            "technically.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "from docx import Document\n",
        "from docx.shared import Pt, Inches\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "\n",
        "\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.docstore.document import Document as LC_Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "\n",
        "# Setup API Keys\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAicSbl9wnfKplfF6tFaq1CGk4u_zpOKmc\"  # Replace with your Gemini API key\n",
        "google_api_key = \"AIzaSyAFSUrPJr1DnA4GDOa1HC--6OAX8v3SUPs\"          # Google Custom Search API key\n",
        "cx = \"648b68ccf3907408b\"                    # Your Custom Search Engine ID\n",
        "\n",
        "# Initialize the LLM (Gemini 2.0 Flash)\n",
        "llm = GoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def google_image_search(query, api_key, cx, num=1):\n",
        "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"key\": api_key,\n",
        "        \"cx\": cx,\n",
        "        \"searchType\": \"image\",\n",
        "        \"num\": num\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    response.raise_for_status()\n",
        "    results = response.json()\n",
        "    if \"items\" in results and len(results[\"items\"]) > 0:\n",
        "        return results[\"items\"][0][\"link\"]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "notes_per_section = {}      # section index -> detailed notes text\n",
        "images_per_section = {}     # section index -> image URL\n",
        "\n",
        "# Process each section\n",
        "for idx, section in enumerate(formatted_sections, start=1):\n",
        "    print(f\"\\nProcessing Section {idx}...\")\n",
        "\n",
        "    #  Stage 1: Generate Detailed Lecture Notes for the section\n",
        "    text_prompt = f\"\"\"\n",
        "Please transform the following text section into detailed, elaborated lecture notes for students.\n",
        "Expand on the content by adding clear explanations and extra details to enhance understanding.\n",
        "Format your output as follows:\n",
        "- Start each main heading with \"# \" (title case).\n",
        "- Start each subheading with \"## \" on a new line.\n",
        "- Use \"* \" to begin each bullet point on its own line.\n",
        "- Write normal paragraphs as plain text.\n",
        "- Use proper Unicode for any subscripts/superscripts (e.g., output CO₂ as \"CO₂\").\n",
        "- Do not include any extra text or instructions beyond the elaborated lecture notes.\n",
        "\n",
        "Text:\n",
        "{section}\n",
        "    \"\"\"\n",
        "    detailed_notes = llm(text_prompt).strip()\n",
        "    print(\"Detailed notes generated.\")\n",
        "    notes_per_section[idx] = detailed_notes\n",
        "\n",
        "    # Stage 2: Generate Image Search Query for the section\n",
        "    image_prompt = f\"\"\"\n",
        "Based on the following section's heading and key points, generate a concise image search query that would return a relevant image for this topic.\n",
        "\n",
        "Section:\n",
        "{section}\n",
        "\n",
        "Image Search Query:\n",
        "    \"\"\"\n",
        "    image_query = llm(image_prompt).strip()\n",
        "    print(\"Generated image query:\", image_query)\n",
        "\n",
        "    # Use the generated query to search for an image.\n",
        "    image_url = google_image_search(image_query, google_api_key, cx)\n",
        "    print(\"Retrieved image URL:\", image_url)\n",
        "    images_per_section[idx] = image_url\n",
        "\n",
        "\n",
        "    if image_url:\n",
        "        try:\n",
        "            img_response = requests.get(image_url)\n",
        "            img_response.raise_for_status()\n",
        "            img_data = BytesIO(img_response.content)\n",
        "            img = Image.open(img_data)\n",
        "            display(img)\n",
        "        except Exception as e:\n",
        "            print(\"Error displaying image:\", e)\n",
        "    else:\n",
        "        print(\"PTO\")\n",
        "\n",
        "#  Concatenate all detailed notes for summarization\n",
        "all_notes = \"\"\n",
        "for idx in sorted(notes_per_section.keys()):\n",
        "    all_notes += \"\\n\" + notes_per_section[idx]\n",
        "\n",
        "#  Generate an overall summary using the detailed notes\n",
        "doc_for_summary = LC_Document(page_content=all_notes)\n",
        "custom_summarize_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=(\n",
        "        \"You are an expert in this subject. Read the text below and produce a comprehensive, detailed summary. \"\n",
        "        \"Include the main points, examples, and any significant takeaways. \"\n",
        "        \"Make sure the summary is clear, concise, and covers all important details.\\n\\n\"\n",
        "        \"TEXT:\\n{text}\\n\\n\"\n",
        "        \"DETAILED SUMMARY:\"\n",
        "    ),\n",
        ")\n",
        "summarize_chain = load_summarize_chain(\n",
        "    llm,\n",
        "    chain_type=\"stuff\",\n",
        "    prompt=custom_summarize_prompt,\n",
        ")\n",
        "summary = summarize_chain.run([doc_for_summary])\n",
        "print(\"\\n=== Overall Summary ===\\n\", summary)\n",
        "\n",
        "# Create the Summary Word Document (text only)\n",
        "summary_doc = Document()\n",
        "\n",
        "# Style the summary document\n",
        "summary_heading_style = summary_doc.styles[\"Heading 1\"]\n",
        "summary_heading_style.font.name = \"Arial\"\n",
        "summary_heading_style.font.size = Pt(16)\n",
        "summary_heading_style.font.bold = True\n",
        "summary_normal_style = summary_doc.styles[\"Normal\"]\n",
        "summary_normal_style.font.name = \"Calibri\"\n",
        "summary_normal_style.font.size = Pt(11)\n",
        "\n",
        "summary_doc.add_paragraph(\"Summary\", style=\"Heading 1\").alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "\n",
        "# Split the summary into sentences and add each as a paragraph for readability.\n",
        "sentences = re.split(r'(?<=[.!?])\\s+', summary.strip())\n",
        "for sentence in sentences:\n",
        "    if sentence.strip():\n",
        "        p = summary_doc.add_paragraph(sentence.strip(), style=\"Normal\")\n",
        "        p.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
        "\n",
        "summary_doc_filename = \"fromsoft_summary.docx\"\n",
        "summary_doc.save(summary_doc_filename)\n",
        "print(f\"Summary document generated: {summary_doc_filename}\")\n",
        "\n",
        "# Create the Main Detailed Notes Word Document with Images\n",
        "main_doc = Document()\n",
        "\n",
        "# Customize styles for the main document.\n",
        "main_heading1 = main_doc.styles[\"Heading 1\"]\n",
        "main_heading1.font.name = \"Arial\"\n",
        "main_heading1.font.size = Pt(16)\n",
        "main_heading1.font.bold = True\n",
        "\n",
        "main_heading2 = main_doc.styles[\"Heading 2\"]\n",
        "main_heading2.font.name = \"Arial\"\n",
        "main_heading2.font.size = Pt(14)\n",
        "main_heading2.font.bold = True\n",
        "\n",
        "main_normal = main_doc.styles[\"Normal\"]\n",
        "main_normal.font.name = \"Calibri\"\n",
        "main_normal.font.size = Pt(11)\n",
        "\n",
        "# For each section, add the detailed notes and then the image.\n",
        "for idx in sorted(notes_per_section.keys()):\n",
        "    # Add the detailed notes.\n",
        "    section_text = notes_per_section[idx]\n",
        "\n",
        "    # Split the section text into lines; assume the first line is a heading.\n",
        "    lines = section_text.strip().split(\"\\n\")\n",
        "    if lines:\n",
        "        heading_text = lines[0].lstrip(\"# \").strip()\n",
        "        main_doc.add_heading(heading_text, level=1)\n",
        "        for line in lines[1:]:\n",
        "            if line.strip():\n",
        "                # Remove bullet markers if present.\n",
        "                text = line.lstrip(\"* \").strip()\n",
        "                main_doc.add_paragraph(text, style=\"Normal\")\n",
        "\n",
        "    # Add the corresponding image if available.\n",
        "    image_url = images_per_section.get(idx)\n",
        "    if image_url:\n",
        "        try:\n",
        "            img_response = requests.get(image_url)\n",
        "            img_response.raise_for_status()\n",
        "            img_data = BytesIO(img_response.content)\n",
        "            main_doc.add_picture(img_data, width=Inches(4))\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding image for section {idx}:\", e)\n",
        "    else:\n",
        "        main_doc.add_paragraph(\"No image available for this section.\", style=\"Normal\")\n",
        "\n",
        "    # Add a page break after each section.\n",
        "    main_doc.add_page_break()\n",
        "\n",
        "main_doc_filename = \"fromsoft_notes.docx\"\n",
        "main_doc.save(main_doc_filename)\n",
        "print(f\"Main detailed notes document generated: {main_doc_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEFpnNkOib4-",
        "outputId": "1b2565ab-ccb0-434c-b5d0-035d8b50a9a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Section 1...\n",
            "Detailed notes generated.\n",
            "Generated image query: **\"Chrome Dino game reinforcement learning DQN\"**\n",
            "Retrieved image URL: None\n",
            "PTO\n",
            "\n",
            "=== Overall Summary ===\n",
            " This document details the implementation of Reinforcement Learning (RL) agents, specifically Deep Q-Network (DQN) and Quantile Regression DQN (QR-DQN), to play the Chrome Dino game. The goal was to train agents to survive as long as possible and maximize their score in the game, which involves jumping over cacti and ducking under pterodactyls.\n",
            "\n",
            "**1. Introduction to the Chrome Dino Game and Reinforcement Learning Implementation:**\n",
            "\n",
            "*   The Chrome Dino game is an endless runner with increasing speed, requiring the player to avoid obstacles.\n",
            "*   An RL agent was implemented to play the game automatically, learning through trial and error, receiving rewards for success and penalties for failure. The agent aims to learn an optimal policy to maximize cumulative reward.\n",
            "\n",
            "**2. Environment Details:**\n",
            "\n",
            "*   The environment consists of the game screen, including obstacles (cacti and pterodactyls), increasing game speed, and fixed jump mechanics.\n",
            "*   The agent interacts by choosing actions: Jump, Duck, or Stand.\n",
            "\n",
            "**3. Objective:**\n",
            "\n",
            "*   Primary objective: Survive as long as possible.\n",
            "*   Secondary objective: Maximize the score.\n",
            "*   Achieved by learning to choose optimal actions.\n",
            "\n",
            "**4. Dataset Description:**\n",
            "\n",
            "*   RL doesn't use a pre-existing labeled dataset. The \"dataset\" is generated dynamically through interaction with the environment.\n",
            "*   The core data sample is a *transition tuple*: (s, a, r, s', done).\n",
            "    *   s: Current state (stacked grayscale game frames).\n",
            "    *   a: Action taken (Jump, Duck, Stand).\n",
            "    *   r: Reward received (+1 for passing an obstacle, -1 for collision).\n",
            "    *   s': Next state after the action.\n",
            "    *   done: Boolean indicating game over.\n",
            "*   State is represented by stacked grayscale frames to perceive motion and reduce dimensionality.\n",
            "*   A replay buffer stores past experiences (transition tuples) to break correlations and improve learning stability.\n",
            "\n",
            "**5. Recent Technique Architecture:**\n",
            "\n",
            "*   DQN and QR-DQN algorithms were used, implemented with the Stable-Baselines3 library.\n",
            "*   Input: Stack of 4 grayscale, resized (4 x 64 x 128) game frames.\n",
            "*   Backbone: Convolutional Neural Network (CNN) via CnnPolicy in Stable-Baselines3, suitable for image data feature extraction.\n",
            "*   CNN architecture: 3 Convolutional Layers and Fully Connected Layers.\n",
            "*   QR-DQN: Estimates the distribution of rewards, outputting multiple quantiles per action, providing uncertainty awareness and robustness.\n",
            "\n",
            "**6. Hyperparameters:**\n",
            "\n",
            "*   Learning rate: 1e-4 (0.0001).\n",
            "*   Batch size: 32.\n",
            "*   Replay buffer size: 100,000.\n",
            "*   Exploration Strategy: Epsilon-Greedy.\n",
            "    *   Initial epsilon: 1.0.\n",
            "    *   Final epsilon: 0.02.\n",
            "    *   Exploration decay: 10% of total timesteps.\n",
            "*   Train frequency: every 4 steps.\n",
            "*   Target network update interval: 10,000 steps.\n",
            "*   Discount factor (γ): 0.99.\n",
            "*   Total timesteps: 2 million.\n",
            "*   Eval callback frequency: 20,000 steps.\n",
            "\n",
            "**7. Comparison of Hyperparameters:**\n",
            "\n",
            "*   Identical hyperparameters were used for DQN and QR-DQN to ensure a fair comparison, isolating the impact of architectural differences.\n",
            "*   Key difference: DQN outputs a single Q-value per action, while QR-DQN outputs 51 quantiles, representing the distribution of possible returns.\n",
            "*   DQN performed better than QR-DQN in this constrained setting, suggesting that distributional modeling may not be necessary in this relatively simple environment.\n",
            "\n",
            "**8. Metrics Used:**\n",
            "\n",
            "*   ep_rew_mean: Mean reward per episode (primary metric).\n",
            "*   ep_len_mean: Mean episode length (fixed to 125).\n",
            "*   n_updates: Total number of learning updates.\n",
            "*   loss: Bellman (DQN) or quantile regression (QR-DQN) loss.\n",
            "*   Maximum game score achieved (measured manually).\n",
            "\n",
            "**9. Result Analysis and Interpretation:**\n",
            "\n",
            "*   DQN: Converged at episode reward ≈ 17, reached maximum score: 900.\n",
            "*   QR-DQN: Converged at episode reward ≈ 13.1, reached maximum score: 650.\n",
            "*   DQN outperformed QR-DQN, suggesting distributional modeling wasn't beneficial in this simple environment.\n",
            "*   Possible reasons: Simpler environment, QR-DQN's slower convergence due to a more complex objective, and DQN's max-over-actions Q-estimate was more decisive.\n",
            "\n",
            "**10. Conclusions:**\n",
            "\n",
            "*   Both DQN and QR-DQN learned to play the Chrome Dino game.\n",
            "*   DQN outperformed QR-DQN under identical hyperparameter settings.\n",
            "*   QR-DQN may need longer training or tuning in low-complexity environments.\n",
            "*   Reinforcement Learning can effectively master pixel-based games with structured rewards using deep Q-learning methods.\n",
            "*   Training logs, TensorBoard results, model weights, and evaluation GIFs are available for further analysis. A report and presentation summarize the methods, metrics, and conclusions.\n",
            "\n",
            "**Significant Takeaways:**\n",
            "\n",
            "*   While QR-DQN is generally considered more robust in complex environments, the simpler DQN algorithm performed better in the Chrome Dino game. This highlights the importance of considering the complexity of the environment when choosing an RL algorithm.  Distributional RL methods may not always be necessary or beneficial.\n",
            "*   The study demonstrates the effectiveness of deep reinforcement learning in mastering pixel-based games, even with relatively simple reward structures.\n",
            "*   The availability of training logs, models, and evaluation data promotes reproducibility and further research.\n",
            "Summary document generated: fromsoft_summary.docx\n",
            "Main detailed notes document generated: fromsoft_notes.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('final_notes_with_images_new.docx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "4v5kf1hSjWnP",
        "outputId": "e6a74cdc-fb17-46a8-eb2f-83e74c29e534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Cannot find file: final_notes_with_images_new.docx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ef61565cece3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'final_notes_with_images_new.docx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: final_notes_with_images_new.docx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I4CrKdRAjnpD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}